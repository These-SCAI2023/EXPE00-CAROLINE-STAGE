{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7358c461",
   "metadata": {},
   "source": [
    "du 20/04/2023 au 25/04/2022     \n",
    "notebook 8   \n",
    "objectif : faire une synthèse des observations sur l'entity linker de Spacy   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15946b83",
   "metadata": {},
   "source": [
    "# Test du sytème d'entity linking de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673f0e4",
   "metadata": {},
   "source": [
    "Nous définissons deux fonctions, une avec le modèle de langue anglais \"en_core_web_md\", l'autre avec le modèle de langue français \"fr_core_news_lg\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11391b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  # version 3.5\n",
    "# initialize language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5def8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read (texte):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "# add pipeline (declared through entry_points in setup.py)\n",
    "    nlp.add_pipe(\"entityLinker\", last=True)\n",
    "    doc = nlp(texte)\n",
    "# returns all entities in the whole document\n",
    "    all_linked_entities = doc._.linkedEntities\n",
    "# iterates over sentences and prints linked entities\n",
    "    for sent in doc.sents:\n",
    "        sent._.linkedEntities.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff7c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4beb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(texte): \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(texte)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b6a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/dorle/Documents/EXPE00-CAROLINE-STAGE/ELTeC-eng_nonref_nonocr/AGUILAR_home-influence\\AGUILAR_Kraken\\AGUILAR_home-influence_Kraken.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1029995 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[0;32m     15\u001b[0m txt\u001b[38;5;241m=\u001b[39mlire_fichier(path)  \n\u001b[1;32m---> 16\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     17\u001b[0m read(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:999\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    980\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    983\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    984\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    985\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1090\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1079\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1080\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m   1081\u001b[0m     )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1029995 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe(\"entityLinker\", last=True)\n",
    "\n",
    "def lire_fichier(chemin):\n",
    "    with open (chemin, \"r\", encoding=\"utf-8\") as fichier:\n",
    "        texte=fichier.read()\n",
    "        return texte\n",
    "\n",
    "path_corpora = path_corpora = 'C:/Users/dorle/Documents/EXPE00-CAROLINE-STAGE/ELTeC-eng_nonref_nonocr/AGUILAR_home-influence/*/*.txt'\n",
    "for path in glob.glob(path_corpora):\n",
    "    print(path)\n",
    "    txt=lire_fichier(path)  \n",
    "    doc = nlp(txt) \n",
    "    read(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7356ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
