{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60259ae0",
   "metadata": {},
   "source": [
    "04/05/2023      \n",
    "notebook     \n",
    "objectif : faire le bilan sous forme de diagrammes en bâton cumulés et diagramme de Wenn pour le corpus anglais .    \n",
    "contexte : suite de \"testELSpacy_venn_baton2.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f51eb",
   "metadata": {},
   "source": [
    "# Evaluation comparative  du système d'entity linking de SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad50669",
   "metadata": {},
   "source": [
    "## I-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy  # version 3.5\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def lire_fichier(chemin):\n",
    "    with open (chemin, \"r\", encoding=\"utf-8\") as fichier:\n",
    "        texte=fichier.read()\n",
    "        return texte\n",
    "def stocker(chemin, contenu):\n",
    "    w =open(chemin, \"w\")\n",
    "    w.write(json.dumps(contenu , indent = 2))\n",
    "    w.close()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe(\"entityLinker\", last=True)\n",
    "\n",
    "path_corpora = \"C:/Users/dorle/Documents/EXPE00-CAROLINE-STAGE/ELTeC-eng_nonref_nonocr/*/*/*.txt\"\n",
    "\n",
    "for path in glob.glob(path_corpora):\n",
    "    print(path)\n",
    "    txt=lire_fichier(path)\n",
    "    #doc = nlp(txt[:5000])  \n",
    "    doc = nlp(txt) \n",
    "    i=0\n",
    "    dico_sent_tok ={}\n",
    "    \n",
    "    \n",
    "    for ent in doc._.linkedEntities: \n",
    "            ide = \"ID\"+str(i)\n",
    "            url_00 = ent.get_url()\n",
    "            label_00=ent.get_label()\n",
    "            description=ent.get_description()\n",
    "            span_00=ent.get_span()\n",
    "            span_oo=str(span_00)\n",
    "            doc2=nlp(span_oo)\n",
    "            for en in doc2.ents:#Pour le typage avec les labels de l'entity linking\n",
    "                \n",
    "                # print(en.label_)\n",
    "                dico_sent_tok[ide]={}\n",
    "                dico_sent_tok[ide][\"url\"]=url_00\n",
    "                dico_sent_tok[ide][\"span\"]=span_oo\n",
    "                dico_sent_tok[ide][\"label\"]=label_00\n",
    "                dico_sent_tok[ide][\"description\"]=description\n",
    "                dico_sent_tok[ide][\"type\"]=en.label_#Pour le typage avec les labels de l'entity linking\n",
    "                i+=1\n",
    "            stocker(\"%s_entity-linker-en_core_web_md.json\"%path,dico_sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60164d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import glob\n",
    "\n",
    "def Venn (chemin):\n",
    "    # spécifiez le chemin vers votre dossier contenant les fichiers\n",
    "    path_corpora = \"../DATA/DATA-Fra_spaCy3.3.1_CONCAT/ADAM/*/*/*.json\"\n",
    "\n",
    "    # créez une liste pour stocker les ensembles d'URL communs\n",
    "    liste = []\n",
    "    liste_fichier = []\n",
    "\n",
    "    # bouclez à travers tous les fichiers dans le dossier\n",
    "    for nom_fichier in glob.glob(chemin):\n",
    "        if \"entity-linker\" in str(nom_fichier):\n",
    "            liste_fichier= ['Kraken', 'le texte de référence', 'Tesseract binarisé', 'le texte de référence', 'Tesseract PNG','le texte de référence', 'TesseractFra PNG', 'le texte de référence']\n",
    "            # ouvrez le fichier et chargez les dictionnaires JSON\n",
    "            with open(nom_fichier) as f:\n",
    "                dictionnaires = json.load(f)\n",
    "\n",
    "            # créez un ensemble pour les URL du fichier courant\n",
    "            set_url = set()\n",
    "\n",
    "            # bouclez à travers tous les dictionnaires dans le fichier\n",
    "            for cle, dico in dictionnaires.items():\n",
    "                # obtenez l'URL du dictionnaire\n",
    "                url = dico.get(\"url\")\n",
    "                # ajoutez l'url à l'ensemble des URL\n",
    "                set_url.add(url)\n",
    "\n",
    "            # ajoutez l'ensemble d'URL du fichier courant à la liste\n",
    "            liste.append(set_url)\n",
    "\n",
    "    arr = None\n",
    "    # bouclez à travers les paires d'ensembles d'URL dans la liste\n",
    "    for i in range(len(liste) - 1):\n",
    "        ocr = liste[i]\n",
    "        nb_ocr = len(ocr)\n",
    "        reference = liste[i+1]\n",
    "        nb_reference = len (reference)\n",
    "        intersection = ocr.intersection(reference)\n",
    "        nb_intersection = len(intersection)\n",
    "        element_propre_ocr = len([element for element in ocr if element not in reference]) \n",
    "        element_propre_reference = len([element for element in reference if element not in ocr])\n",
    "        nom = liste_fichier[i]\n",
    "        if arr is None:\n",
    "            arr = np.array([nom, nb_ocr, nb_reference, nb_intersection, element_propre_ocr, element_propre_reference])\n",
    "        else:\n",
    "            arr = np.vstack((arr, [nom, nb_ocr, nb_reference, nb_intersection, element_propre_ocr, element_propre_reference]))\n",
    "\n",
    "        # affiche les noms des fichiers\n",
    "        print(\"Valeurs communes entre\", liste_fichier[i], \"et\", liste_fichier[i+1] )\n",
    " \n",
    "\n",
    "        # vérifie si les ensembles d'URL contiennent des éléments avant de générer le Venn diagram\n",
    "        if ocr and reference:\n",
    "            venn2([ocr, reference], set_labels=(str(liste_fichier[i]), str(liste_fichier[i+1])))\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Ensemble d'URL vide pour au moins un fichier\")\n",
    "\n",
    "    print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019f2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy  # version 3.5\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lire_fichier(chemin):\n",
    "    with open (chemin, \"r\", encoding=\"utf-8\") as fichier:\n",
    "        texte=fichier.read()\n",
    "        return texte\n",
    "def stocker(chemin, contenu):\n",
    "    w =open(chemin, \"w\")\n",
    "    w.write(json.dumps(contenu , indent = 2))\n",
    "    w.close()\n",
    "def batons (chemin, modele) :\n",
    "    url_communs = []\n",
    "    liste_loc = []\n",
    "    liste_autre = []\n",
    "    liste_a = []\n",
    "    liste_b = []\n",
    "\n",
    "    nlp = spacy.load(modele)\n",
    "    nlp.add_pipe(\"entityLinker\", last=True)\n",
    "\n",
    "    \n",
    "\n",
    "    for path in glob.glob(chemin):\n",
    "        #print(path)\n",
    "        txt=lire_fichier(path)\n",
    "        #doc = nlp(txt[:5000]) \n",
    "        doc = nlp(txt)\n",
    "        i=0\n",
    "        dico_sent_tok ={}\n",
    "        liste_loc = []\n",
    "        liste_autre = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        for ent in doc._.linkedEntities: \n",
    "            ide = \"ID\"+str(i)\n",
    "            url_00 = ent.get_url()\n",
    "            label_00=ent.get_label()\n",
    "            description=ent.get_description()\n",
    "            span_00=ent.get_span()\n",
    "            span_oo=str(span_00)\n",
    "            doc2=nlp(span_oo)\n",
    "            for en in doc2.ents:#Pour le typage avec les labels de l'entity linking\n",
    "                \n",
    "                # print(en.label_)\n",
    "                dico_sent_tok[ide]={}\n",
    "                dico_sent_tok[ide][\"url\"]=url_00\n",
    "                dico_sent_tok[ide][\"span\"]=span_oo\n",
    "                dico_sent_tok[ide][\"label\"]=label_00\n",
    "                dico_sent_tok[ide][\"description\"]=description\n",
    "                dico_sent_tok[ide][\"type\"]=en.label_#Pour le typage avec les labels de l'entity linking\n",
    "                i+=1\n",
    "                \n",
    "                if dico_sent_tok[ide][\"type\"] == \"LOC\":\n",
    "                    #dico_sent_tok[ide][\"type\"] = True\n",
    "                    liste_loc.append(dico_sent_tok[ide])\n",
    "                else:\n",
    "                    #dico_sent_tok[ide][\"type\"] = False\n",
    "                    liste_autre.append(dico_sent_tok)\n",
    "        a=len(liste_loc)\n",
    "        # nb de dictionnaire qui contiennent des loc\n",
    "        b=len(liste_autre)\n",
    "        #nb de dictionnaire qui ne contiennent pas des loc\n",
    "        liste_a.append(a)\n",
    "        liste_b.append(b)\n",
    "        stocker(\"%s_entity-linker.json\"%path,dico_sent_tok)\n",
    "                    \n",
    "\n",
    "    largeur_barre = 0.8\n",
    "\n",
    "\n",
    "    x = range(len(liste_a)) # position en abscisse des barres\n",
    "\n",
    "    # # Tracé\n",
    "\n",
    "    plt.bar(x, liste_a, width = largeur_barre, color = \"#3ED8C9\")\n",
    "\n",
    "    plt.bar(x, liste_b, width = largeur_barre, bottom = liste_a, color = \"#EDFF91\")\n",
    "\n",
    "    plt.xticks(range(len(liste_a)), ['Kraken', 'référence', 'Tesseract binarisé', 'référence', 'Tesseract PNG','référence'], rotation=90)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b194efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1029995 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path_corpora \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/dorle/Documents/EXPE00-CAROLINE-STAGE/ELTeC-eng_nonref_nonocr/AGUILAR_home-influence/*/*.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbatons\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_corpora\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m, in \u001b[0;36mbatons\u001b[1;34m(chemin, modele)\u001b[0m\n\u001b[0;32m     29\u001b[0m txt\u001b[38;5;241m=\u001b[39mlire_fichier(path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#doc = nlp(txt[:5000]) \u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m dico_sent_tok \u001b[38;5;241m=\u001b[39m{}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:999\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    980\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    983\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    984\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    985\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1090\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1079\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1080\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m   1081\u001b[0m     )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1029995 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "path_corpora = 'C:/Users/dorle/Documents/EXPE00-CAROLINE-STAGE/ELTeC-eng_nonref_nonocr/AGUILAR_home-influence/*/*.txt'\n",
    "batons (path_corpora,\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f21f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
